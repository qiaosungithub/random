training:
    num_epochs: 2000
    warmup_epochs: 200
    batch_size: 2048
    learning_rate: 0.0008
    eval_per_epoch: 100
    visualize_per_epoch: 100
    log_per_step: 100

    # stablize?
    adam_b2: 0.95
    
    # wandb: False # use this to disable wandb
model:
    dropout: 0.15
    n_T: 10 # 100 FM Euler steps
    t_cond_method: "log999" # option: ["log999", "not"]
    # t_cond_method: "not" # option: ["log999", "not"]
    embedding_type: "positional"
fid:
    fid_per_epoch: 100
    num_samples: 50000
dataset:
    num_workers: 64 # NOTE: On V2, if you don't use num_workers=64, sometimes the code will exit unexpectedly

start: 0.5
load_from: /kmh-nfs-ssd-eu-mount/logs/sqa/fm_linen/20250220_150638_bncik4_kmh-tpuvm-v3-32-13__b_lr_ep_torchvision_r50_eval/checkpoint_48000
just_evaluate: True # use this to just evaluate the model (without training)